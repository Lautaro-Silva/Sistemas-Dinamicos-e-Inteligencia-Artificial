{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0460a24-30d3-4515-b916-e62012713ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2cd8706-a40f-40ce-ba61-20679a859a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Señales de muones: torch.Size([10000, 1, 1000]), targets: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# CONFIG\n",
    "# -----------------------\n",
    "pulse_samples = 1000     # n_feat_muon\n",
    "n_feat_frec = 2500       # n_feat_frec\n",
    "batch_size = 256\n",
    "epochs = 80\n",
    "lr = 1e-3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_classes = 21\n",
    "\n",
    "# Paths a tus datasets\n",
    "muon_dataset_path = \"muon_dataset_segmented_v3.npz\"\n",
    "simulated_npz_path = \"Espectros/spectrum_simulated.npz\"\n",
    "real_npz_path = \"Espectros/spectrum_real.npz\"\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# CARGA DE DATASET DE MUONES\n",
    "# -----------------------\n",
    "data = np.load(muon_dataset_path)\n",
    "X_muones = data['X']        # shape (num_pulsos, pulse_samples)\n",
    "y_muones = data['y']        # shape (num_pulsos,)\n",
    "\n",
    "# -----------------------\n",
    "# Normalizar pulsos de muones al mismo rango\n",
    "# -----------------------\n",
    "# Tomar el máximo global de todos los pulsos\n",
    "max_global = X_muones.max()\n",
    "\n",
    "# Escalar todos los pulsos para que el pulso con mayor valor llegue a 1\n",
    "X_muones = X_muones / (max_global + 1e-8)  # evitar división por cero\n",
    "\n",
    "# Convertir a tensores PyTorch\n",
    "X_muones = torch.tensor(X_muones, dtype=torch.float32).unsqueeze(1)  # (N,1,pulse_samples)\n",
    "y_muones = torch.tensor(y_muones, dtype=torch.long).squeeze()         # (N,)\n",
    "\n",
    "print(f\"Señales de muones: {X_muones.shape}, targets: {y_muones.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# CARGA DE ESPECTROS\n",
    "# -----------------------\n",
    "def load_npz(path):\n",
    "    data = np.load(path)\n",
    "    freq = data['freq']\n",
    "    att = data['attenuation_db']\n",
    "    return torch.tensor(freq, dtype=torch.float32), torch.tensor(att, dtype=torch.float32)\n",
    "\n",
    "sim_freq, H_sim = load_npz(simulated_npz_path)\n",
    "real_freq, H_real = load_npz(real_npz_path)\n",
    "\n",
    "# -----------------------\n",
    "# Normalizar espectros simulados H_sim de 0 a 1\n",
    "# -----------------------\n",
    "H_sim_min = H_sim.min()\n",
    "H_sim_max = H_sim.max()\n",
    "H_sim_norm = (H_sim - H_sim_min) / (H_sim_max - H_sim_min + 1e-8)  # evitar división por cero\n",
    "\n",
    "# Repetir para cada ejemplo\n",
    "H_sim_batch = H_sim_norm.repeat(X_muones.shape[0], 1)  # (N, n_feat_frec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6429a732-4d92-4983-b7ee-9df488b5480c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TwoBranchNet                             [1, 21]                   --\n",
       "├─Sequential: 1-1                        [1, 256]                  --\n",
       "│    └─Conv1d: 2-1                       [1, 64, 1000]             1,024\n",
       "│    └─BatchNorm1d: 2-2                  [1, 64, 1000]             128\n",
       "│    └─LeakyReLU: 2-3                    [1, 64, 1000]             --\n",
       "│    └─MaxPool1d: 2-4                    [1, 64, 500]              --\n",
       "│    └─Conv1d: 2-5                       [1, 128, 500]             57,472\n",
       "│    └─BatchNorm1d: 2-6                  [1, 128, 500]             256\n",
       "│    └─LeakyReLU: 2-7                    [1, 128, 500]             --\n",
       "│    └─MaxPool1d: 2-8                    [1, 128, 250]             --\n",
       "│    └─Conv1d: 2-9                       [1, 256, 250]             164,096\n",
       "│    └─BatchNorm1d: 2-10                 [1, 256, 250]             512\n",
       "│    └─LeakyReLU: 2-11                   [1, 256, 250]             --\n",
       "│    └─AdaptiveAvgPool1d: 2-12           [1, 256, 1]               --\n",
       "│    └─Flatten: 2-13                     [1, 256]                  --\n",
       "├─Sequential: 1-2                        [1, 32]                   --\n",
       "│    └─Linear: 2-14                      [1, 128]                  320,128\n",
       "│    └─LeakyReLU: 2-15                   [1, 128]                  --\n",
       "│    └─Linear: 2-16                      [1, 64]                   8,256\n",
       "│    └─LeakyReLU: 2-17                   [1, 64]                   --\n",
       "│    └─Linear: 2-18                      [1, 32]                   2,080\n",
       "│    └─LeakyReLU: 2-19                   [1, 32]                   --\n",
       "├─Sequential: 1-3                        [1, 21]                   --\n",
       "│    └─Linear: 2-20                      [1, 256]                  73,984\n",
       "│    └─LeakyReLU: 2-21                   [1, 256]                  --\n",
       "│    └─Linear: 2-22                      [1, 128]                  32,896\n",
       "│    └─LeakyReLU: 2-23                   [1, 128]                  --\n",
       "│    └─Dropout: 2-24                     [1, 128]                  --\n",
       "│    └─Linear: 2-25                      [1, 21]                   2,709\n",
       "==========================================================================================\n",
       "Total params: 663,541\n",
       "Trainable params: 663,541\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 71.22\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 3.08\n",
       "Params size (MB): 2.65\n",
       "Estimated Total Size (MB): 5.75\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------\n",
    "# DEFINICIÓN DE LA RED\n",
    "# -----------------------\n",
    "class TwoBranchNet(nn.Module):\n",
    "    def __init__(self, n_feat_muon=1000, n_feat_frec=2500, n_classes=21):\n",
    "        super(TwoBranchNet, self).__init__()\n",
    "        \n",
    "        # --- Rama CNN Muones ---\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=15, padding='same'),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool1d(2),\n",
    "\n",
    "            nn.Conv1d(64, 128, kernel_size=7, padding='same'),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.1),         \n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "\n",
    "            nn.Conv1d(128, 256, kernel_size=5, padding='same'),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.AdaptiveAvgPool1d(1),  # (batch, 256, 1)\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Rama 2: Respuesta en frecuencia simulada (MLP)\n",
    "        self.mlp_branch = nn.Sequential(\n",
    "            nn.Linear(n_feat_frec, 128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "        # --- Cabezal de fusión ---\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(256 + 32, 256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_muon, x_freq):\n",
    "        v1 = self.cnn_branch(x_muon)\n",
    "        v1 = v1.view(v1.size(0), -1)\n",
    "        v2 = self.mlp_branch(x_freq)\n",
    "        merged = torch.cat([v1, v2], dim=1)\n",
    "        out = self.head(merged)\n",
    "        return out\n",
    "\n",
    "# -----------------------\n",
    "# Inicializar modelo\n",
    "# -----------------------\n",
    "model = TwoBranchNet(n_feat_muon=pulse_samples, n_feat_frec=n_feat_frec, n_classes=n_classes).to(device)\n",
    "summary(model, input_size=[(1,1,1000), (1,2500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6ae380-d692-4784-8c85-af792c41c208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo por clase: [18, 11, 21, 74, 271, 939, 2078, 2151, 37, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Pesos por clase: [0.0555555559694767, 0.09090909361839294, 0.0476190485060215, 0.013513513840734959, 0.0036900369450449944, 0.0010649627074599266, 0.0004812319530174136, 0.00046490004751831293, 0.027027027681469917, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Train size: 5600, Val size: 2400, Test size: 2000\n",
      "\n",
      "Verificando balanceo de batches...\n",
      "\n",
      "Batch 0 (256 samples):\n",
      "  Clase 0: 31 samples (12.1%)\n",
      "  Clase 1: 32 samples (12.5%)\n",
      "  Clase 2: 27 samples (10.5%)\n",
      "  Clase 3: 28 samples (10.9%)\n",
      "  Clase 4: 22 samples (8.6%)\n",
      "  Clase 5: 26 samples (10.2%)\n",
      "  Clase 6: 28 samples (10.9%)\n",
      "  Clase 7: 30 samples (11.7%)\n",
      "  Clase 8: 32 samples (12.5%)\n",
      "\n",
      "Batch 1 (256 samples):\n",
      "  Clase 0: 34 samples (13.3%)\n",
      "  Clase 1: 38 samples (14.8%)\n",
      "  Clase 2: 27 samples (10.5%)\n",
      "  Clase 3: 32 samples (12.5%)\n",
      "  Clase 4: 24 samples (9.4%)\n",
      "  Clase 5: 24 samples (9.4%)\n",
      "  Clase 6: 24 samples (9.4%)\n",
      "  Clase 7: 31 samples (12.1%)\n",
      "  Clase 8: 22 samples (8.6%)\n",
      "\n",
      "Batch 2 (256 samples):\n",
      "  Clase 0: 27 samples (10.5%)\n",
      "  Clase 1: 29 samples (11.3%)\n",
      "  Clase 2: 20 samples (7.8%)\n",
      "  Clase 3: 26 samples (10.2%)\n",
      "  Clase 4: 29 samples (11.3%)\n",
      "  Clase 5: 34 samples (13.3%)\n",
      "  Clase 6: 28 samples (10.9%)\n",
      "  Clase 7: 33 samples (12.9%)\n",
      "  Clase 8: 30 samples (11.7%)\n",
      "Epoch 1/80 | Train Loss: 2.2231, Acc: 0.2229 | Val Loss: 3.3918, Acc: 0.0037\n",
      "Epoch 2/80 | Train Loss: 1.6779, Acc: 0.3093 | Val Loss: 2.4268, Acc: 0.2579\n",
      "Epoch 3/80 | Train Loss: 1.6290, Acc: 0.3287 | Val Loss: 2.0549, Acc: 0.4067\n",
      "Epoch 4/80 | Train Loss: 1.5803, Acc: 0.3491 | Val Loss: 3.0429, Acc: 0.1417\n",
      "Epoch 5/80 | Train Loss: 1.5646, Acc: 0.3573 | Val Loss: 1.3934, Acc: 0.5162\n",
      "Epoch 6/80 | Train Loss: 1.5413, Acc: 0.3707 | Val Loss: 2.0982, Acc: 0.4008\n",
      "Epoch 7/80 | Train Loss: 1.5703, Acc: 0.3504 | Val Loss: 2.4427, Acc: 0.3038\n",
      "Epoch 8/80 | Train Loss: 1.5491, Acc: 0.3634 | Val Loss: 1.5115, Acc: 0.4396\n",
      "Epoch 9/80 | Train Loss: 1.5236, Acc: 0.3736 | Val Loss: 1.8996, Acc: 0.4992\n",
      "Epoch 10/80 | Train Loss: 1.5114, Acc: 0.3755 | Val Loss: 1.2155, Acc: 0.5983\n",
      "Epoch 11/80 | Train Loss: 1.4991, Acc: 0.4018 | Val Loss: 1.5984, Acc: 0.5112\n",
      "Epoch 12/80 | Train Loss: 1.4891, Acc: 0.4002 | Val Loss: 2.9696, Acc: 0.1896\n",
      "Epoch 13/80 | Train Loss: 1.4992, Acc: 0.3961 | Val Loss: 1.7193, Acc: 0.4842\n",
      "Epoch 14/80 | Train Loss: 1.4856, Acc: 0.3907 | Val Loss: 1.3587, Acc: 0.5171\n",
      "Epoch 15/80 | Train Loss: 1.5007, Acc: 0.3839 | Val Loss: 1.2515, Acc: 0.5608\n",
      "Epoch 16/80 | Train Loss: 1.4937, Acc: 0.3925 | Val Loss: 1.3384, Acc: 0.5600\n",
      "Epoch 17/80 | Train Loss: 1.4773, Acc: 0.3982 | Val Loss: 1.2098, Acc: 0.5546\n",
      "Epoch 18/80 | Train Loss: 1.4698, Acc: 0.4050 | Val Loss: 1.6846, Acc: 0.5800\n",
      "Epoch 19/80 | Train Loss: 1.4914, Acc: 0.3950 | Val Loss: 1.6581, Acc: 0.5746\n",
      "Epoch 20/80 | Train Loss: 1.4487, Acc: 0.4096 | Val Loss: 1.3897, Acc: 0.5704\n",
      "Epoch 21/80 | Train Loss: 1.4733, Acc: 0.3964 | Val Loss: 1.3225, Acc: 0.5592\n",
      "Epoch 22/80 | Train Loss: 1.4807, Acc: 0.3975 | Val Loss: 1.2625, Acc: 0.5271\n",
      "Epoch 23/80 | Train Loss: 1.5013, Acc: 0.3898 | Val Loss: 1.3835, Acc: 0.5717\n",
      "Epoch 24/80 | Train Loss: 1.4754, Acc: 0.3982 | Val Loss: 1.3943, Acc: 0.5854\n",
      "Epoch 25/80 | Train Loss: 1.4512, Acc: 0.4159 | Val Loss: 1.2562, Acc: 0.5654\n",
      "Epoch 26/80 | Train Loss: 1.4775, Acc: 0.4000 | Val Loss: 1.3682, Acc: 0.5846\n",
      "Epoch 27/80 | Train Loss: 1.4678, Acc: 0.4009 | Val Loss: 1.4299, Acc: 0.5896\n",
      "Epoch 28/80 | Train Loss: 1.4685, Acc: 0.4114 | Val Loss: 1.4432, Acc: 0.5967\n",
      "Epoch 29/80 | Train Loss: 1.4655, Acc: 0.4082 | Val Loss: 1.2312, Acc: 0.5613\n",
      "Epoch 30/80 | Train Loss: 1.4491, Acc: 0.4163 | Val Loss: 1.3831, Acc: 0.5829\n",
      "Epoch 31/80 | Train Loss: 1.4587, Acc: 0.4168 | Val Loss: 1.4152, Acc: 0.5775\n",
      "Epoch 32/80 | Train Loss: 1.4705, Acc: 0.4098 | Val Loss: 1.3991, Acc: 0.5754\n",
      "Epoch 33/80 | Train Loss: 1.4802, Acc: 0.3966 | Val Loss: 1.4274, Acc: 0.5825\n",
      "Epoch 34/80 | Train Loss: 1.4524, Acc: 0.4155 | Val Loss: 1.3618, Acc: 0.5675\n",
      "Epoch 35/80 | Train Loss: 1.4828, Acc: 0.3993 | Val Loss: 1.4032, Acc: 0.5771\n",
      "Epoch 36/80 | Train Loss: 1.4760, Acc: 0.4036 | Val Loss: 1.3725, Acc: 0.5787\n",
      "Epoch 37/80 | Train Loss: 1.4532, Acc: 0.4095 | Val Loss: 1.3738, Acc: 0.5746\n",
      "Epoch 38/80 | Train Loss: 1.4334, Acc: 0.4239 | Val Loss: 1.4493, Acc: 0.5754\n",
      "Epoch 39/80 | Train Loss: 1.4534, Acc: 0.4062 | Val Loss: 1.3910, Acc: 0.5742\n",
      "Epoch 40/80 | Train Loss: 1.4539, Acc: 0.4164 | Val Loss: 1.3656, Acc: 0.5750\n",
      "Epoch 41/80 | Train Loss: 1.4926, Acc: 0.3930 | Val Loss: 1.3322, Acc: 0.5775\n",
      "Epoch 42/80 | Train Loss: 1.4703, Acc: 0.4020 | Val Loss: 1.3578, Acc: 0.5737\n",
      "Epoch 43/80 | Train Loss: 1.4369, Acc: 0.4136 | Val Loss: 1.4455, Acc: 0.5713\n",
      "Epoch 44/80 | Train Loss: 1.4612, Acc: 0.4070 | Val Loss: 1.3686, Acc: 0.5725\n",
      "Epoch 45/80 | Train Loss: 1.4379, Acc: 0.4257 | Val Loss: 1.3808, Acc: 0.5758\n",
      "Epoch 46/80 | Train Loss: 1.4633, Acc: 0.4082 | Val Loss: 1.3763, Acc: 0.5746\n",
      "Epoch 47/80 | Train Loss: 1.4569, Acc: 0.4141 | Val Loss: 1.3870, Acc: 0.5746\n",
      "Epoch 48/80 | Train Loss: 1.4649, Acc: 0.4104 | Val Loss: 1.3618, Acc: 0.5758\n",
      "Epoch 49/80 | Train Loss: 1.4570, Acc: 0.4173 | Val Loss: 1.4319, Acc: 0.5767\n",
      "Epoch 50/80 | Train Loss: 1.4521, Acc: 0.4109 | Val Loss: 1.3643, Acc: 0.5737\n",
      "Epoch 51/80 | Train Loss: 1.4606, Acc: 0.4152 | Val Loss: 1.3682, Acc: 0.5758\n",
      "Epoch 52/80 | Train Loss: 1.4729, Acc: 0.4066 | Val Loss: 1.3504, Acc: 0.5787\n",
      "Epoch 53/80 | Train Loss: 1.4529, Acc: 0.4205 | Val Loss: 1.3818, Acc: 0.5754\n",
      "Epoch 54/80 | Train Loss: 1.4575, Acc: 0.4136 | Val Loss: 1.3736, Acc: 0.5750\n",
      "Epoch 55/80 | Train Loss: 1.4609, Acc: 0.4111 | Val Loss: 1.4098, Acc: 0.5775\n",
      "Epoch 56/80 | Train Loss: 1.4384, Acc: 0.4180 | Val Loss: 1.5063, Acc: 0.5733\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Datasets y DataLoaders\n",
    "# -----------------------\n",
    "\n",
    "dataset = TensorDataset(X_muones, H_sim_batch, y_muones)\n",
    "\n",
    "# Primer split: Train+Val vs Test (80/20)\n",
    "n_total = len(dataset)\n",
    "n_trainval = int(0.8 * n_total)\n",
    "n_test = n_total - n_trainval\n",
    "trainval_dataset, test_dataset = random_split(dataset, [n_trainval, n_test])\n",
    "\n",
    "# Segundo split: Train vs Val (70/30 del 80%)\n",
    "n_train = int(0.7 * n_trainval)\n",
    "n_val  = n_trainval - n_train\n",
    "train_dataset, val_dataset = random_split(trainval_dataset, [n_train, n_val])\n",
    "\n",
    "\n",
    "\n",
    "# Obtener las etiquetas de tu dataset de entrenamiento\n",
    "all_labels = []\n",
    "for i in range(len(train_dataset)):\n",
    "    _, _, label = train_dataset[i]\n",
    "    all_labels.append(label)\n",
    "\n",
    "all_labels = torch.tensor(all_labels)\n",
    "\n",
    "# Calcular el peso para cada clase\n",
    "class_counts = torch.bincount(all_labels, minlength=n_classes)\n",
    "class_weights = 1.0 / class_counts.float()\n",
    "class_weights[class_weights == float('inf')] = 0  # Manejar clases con 0 muestras\n",
    "\n",
    "print(\"Conteo por clase:\", class_counts.tolist())\n",
    "print(\"Pesos por clase:\", class_weights.tolist())\n",
    "\n",
    "# Asignar a cada muestra el peso de su clase\n",
    "sample_weights = [class_weights[label] for label in all_labels]\n",
    "\n",
    "\n",
    "\n",
    "# Crear el WeightedRandomSampler\n",
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),  # Número de muestras a muestrear por época\n",
    "    replacement=True  # IMPORTANTE: Permitir muestreo con reemplazo para clases minoritarias\n",
    ")\n",
    "\n",
    "# DataLoader de entrenamiento con el sampler\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    sampler=weighted_sampler,  # Usar el sampler ponderado\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Val y Test se mantienen igual (sin sampler, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}, Test size: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "def check_sampler_balance(loader, n_classes, num_batches=3):\n",
    "    \"\"\"Verifica que el sampler esté balanceando correctamente\"\"\"\n",
    "    print(\"\\nVerificando balanceo de batches...\")\n",
    "    \n",
    "    for batch_idx, (x_muon, x_freq, y) in enumerate(loader):\n",
    "        if batch_idx >= num_batches:\n",
    "            break\n",
    "        \n",
    "        class_counts = torch.bincount(y, minlength=n_classes)\n",
    "        total_samples = len(y)\n",
    "        \n",
    "        print(f\"\\nBatch {batch_idx} ({total_samples} samples):\")\n",
    "        for class_id in range(n_classes):\n",
    "            count = class_counts[class_id].item()\n",
    "            if count > 0:\n",
    "                percentage = (count / total_samples) * 100\n",
    "                print(f\"  Clase {class_id}: {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "# Verificar el balanceo\n",
    "check_sampler_balance(train_loader, n_classes, num_batches=3)\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Inicialización de modelo, optimizer y scheduler\n",
    "# -----------------------\n",
    "model = TwoBranchNet(n_feat_muon=pulse_samples, n_feat_frec=n_feat_frec, n_classes=n_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Custom Loss: CrossEntropy + Penalización de cero\n",
    "# -----------------------\n",
    "def custom_loss_distance(outputs, targets, min_prob=0.05, alpha=100.0):\n",
    "    \"\"\"\n",
    "    CrossEntropy + penalización más fuerte para clases presentes con baja fracción de predicción.\n",
    "    Penalización decrece suavemente si la fracción aumenta.\n",
    "    \n",
    "    min_prob: fracción mínima esperada de predicciones por clase presente\n",
    "    alpha: escala de penalización\n",
    "    \"\"\"\n",
    "    ce = nn.CrossEntropyLoss()(outputs, targets)\n",
    "    \n",
    "    probs = torch.softmax(outputs, dim=1)\n",
    "    present_classes = torch.unique(targets)\n",
    "\n",
    "    penalty = 0.0\n",
    "    for c in present_classes:\n",
    "        pred_fraction = (probs.argmax(dim=1) == c).float().mean()\n",
    "\n",
    "        # Diferencia respecto al mínimo\n",
    "        diff = torch.relu(min_prob - pred_fraction)\n",
    "\n",
    "        # Penalización cuadrática -> más fuerte si está muy cerca de 0\n",
    "        penalty += diff ** 2  \n",
    "    \n",
    "    penalty = penalty / len(present_classes)\n",
    "    return ce + alpha * penalty\n",
    "\n",
    "\n",
    "def custom_loss_strong(outputs, targets, min_prob=0.05, alpha=100.0): \n",
    "    \"\"\" \n",
    "    CrossEntropy + penalización fuerte si alguna clase presente no se predice. \n",
    "    min_prob: probabilidad mínima que queremos para cada clase presente\n",
    "    \"\"\" \n",
    "    ce = nn.CrossEntropyLoss()(outputs, targets) \n",
    "\n",
    "    probs = torch.softmax(outputs, dim=1) \n",
    "    batch_size = targets.size(0) \n",
    "    present_classes = torch.unique(targets) \n",
    "\n",
    "    penalty = 0.0 \n",
    "    for c in present_classes: # fracción de ejemplos predichos como c \n",
    "        pred_fraction = (probs.argmax(dim=1) == c).float().mean() # penalizamos si pred_fraction < min_prob \n",
    "\n",
    "        penalty += torch.relu(min_prob - pred_fraction) # promedio sobre las clases presentes \n",
    "\n",
    "        penalty = penalty / len(present_classes) \n",
    "        \n",
    "    return ce + alpha * penalty # multiplicador fuerte\n",
    "\n",
    "\n",
    "# Definir una función de pérdida combinada\n",
    "def custom_loss_combined(outputs, targets, class_weights, min_prob=0.05, alpha=0):\n",
    "    \"\"\"\n",
    "    Combina CrossEntropyLoss ponderada con la penalización de fracción de predicción.\n",
    "    \"\"\"\n",
    "    # Entropía cruzada ponderada\n",
    "    ce_loss = nn.CrossEntropyLoss(weight=class_weights)(outputs, targets)\n",
    "\n",
    "    # Tu penalización personalizada (sin cambios)\n",
    "    probs = torch.softmax(outputs, dim=1)\n",
    "    present_classes = torch.unique(targets)\n",
    "    \n",
    "    penalty = 0.0\n",
    "    for c in present_classes:\n",
    "        pred_fraction = (probs.argmax(dim=1) == c).float().mean()\n",
    "        penalty += torch.relu(min_prob - pred_fraction)\n",
    "\n",
    "    penalty = penalty / len(present_classes)\n",
    "    \n",
    "    return ce_loss + alpha * penalty\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Entrenamiento\n",
    "# -----------------------\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss, running_corrects = 0.0, 0\n",
    "    for x_muon_batch, x_freq_batch, y_batch in train_loader:\n",
    "        x_muon_batch = x_muon_batch.to(device)\n",
    "        x_freq_batch = x_freq_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_muon_batch, x_freq_batch)\n",
    "        loss = custom_loss_strong(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * x_muon_batch.size(0)\n",
    "        preds = output.argmax(dim=1)\n",
    "        running_corrects += (preds == y_batch).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / n_train\n",
    "    epoch_acc  = running_corrects / n_train\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accs.append(epoch_acc)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_running_loss, val_running_corrects = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x_muon_batch, x_freq_batch, y_batch in val_loader:\n",
    "            x_muon_batch = x_muon_batch.to(device)\n",
    "            x_freq_batch = x_freq_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            val_output = model(x_muon_batch, x_freq_batch)\n",
    "            val_loss = custom_loss_strong(val_output, y_batch)\n",
    "\n",
    "            val_running_loss += val_loss.item() * x_muon_batch.size(0)\n",
    "            preds = val_output.argmax(dim=1)\n",
    "            val_running_corrects += (preds == y_batch).sum().item()\n",
    "\n",
    "    val_epoch_loss = val_running_loss / n_val\n",
    "    val_epoch_acc  = val_running_corrects / n_val\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accs.append(val_epoch_acc)\n",
    "\n",
    "    scheduler.step(val_epoch_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "          f\"Train Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_epoch_loss:.4f}, Acc: {val_epoch_acc:.4f}\")\n",
    "\n",
    "# -----------------------\n",
    "# Guardar pesos\n",
    "# -----------------------\n",
    "os.makedirs(\"model_weights\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"model_weights/two_branch_net.pth\")\n",
    "print(\"Pesos guardados en 'model_weights/two_branch_net.pth'\")\n",
    "\n",
    "# -----------------------\n",
    "# Evaluación y métricas\n",
    "# -----------------------\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_muon_batch, x_freq_batch, y_batch in test_loader:\n",
    "        x_muon_batch = x_muon_batch.to(device)\n",
    "        x_freq_batch = x_freq_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        output = model(x_muon_batch, x_freq_batch)\n",
    "        preds = output.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=list(range(n_classes)))\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Etiqueta real\")\n",
    "plt.title(\"Matriz de Confusión\")\n",
    "plt.show()\n",
    "\n",
    "# Reporte de clasificación\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n",
    "\n",
    "# -----------------------\n",
    "# Plot de pérdida y accuracy\n",
    "# -----------------------\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"CrossEntropy Loss\")\n",
    "plt.title(\"Pérdida\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_accs, label=\"Train Acc\")\n",
    "plt.plot(val_accs, label=\"Val Acc\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Precisión\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f0a97-1baf-4641-9de1-e951764294d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Evaluación en Test\n",
    "# -----------------------\n",
    "model.eval()\n",
    "test_preds_all = []\n",
    "test_targets_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_muon_batch, x_freq_batch, y_batch in test_loader:\n",
    "        x_muon_batch = x_muon_batch.to(device)\n",
    "        x_freq_batch = x_freq_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        test_output = model(x_muon_batch, x_freq_batch)\n",
    "        preds = test_output.argmax(dim=1)   # <- argmax para clases\n",
    "        test_preds_all.append(preds.cpu())\n",
    "        test_targets_all.append(y_batch.cpu())\n",
    "\n",
    "# Concatenar todos los batches\n",
    "test_preds_all = torch.cat(test_preds_all)\n",
    "test_targets_all = torch.cat(test_targets_all)\n",
    "\n",
    "# Accuracy\n",
    "acc_test = accuracy_score(test_targets_all.numpy(), test_preds_all.numpy())\n",
    "print(f\"Accuracy en Test: {acc_test*100:.2f}%\")\n",
    "\n",
    "# Histograma de clases predichas vs reales\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(test_targets_all.numpy(), bins=np.arange(-0.5, 21.5, 1), alpha=0.6, label=\"Target\", edgecolor='k')\n",
    "plt.hist(test_preds_all.numpy(), bins=np.arange(-0.5, 21.5, 1), alpha=0.6, label=\"Predicción\", edgecolor='k')\n",
    "plt.xlabel(\"Clase\")\n",
    "plt.ylabel(\"Cantidad\")\n",
    "plt.title(\"Distribución de clases en Test\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Matriz de confusión\n",
    "plt.subplot(1,2,2)\n",
    "cm = confusion_matrix(test_targets_all.numpy(), test_preds_all.numpy(), labels=np.arange(21))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(21))\n",
    "disp.plot(ax=plt.gca(), cmap='Blues', colorbar=False)\n",
    "plt.title(\"Matriz de Confusión\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d7a52-5a7a-4b65-8baa-1bf4186203ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13",
   "language": "python",
   "name": "python313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
