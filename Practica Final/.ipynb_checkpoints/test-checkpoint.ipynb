{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d18cd0-d1ca-44ea-b285-fc40542bb872",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sim→Real Counter Predictor — AMIGA electronics (Gain 0 measured S21)\n",
    "--------------------------------------------------------------------\n",
    "\n",
    "Goal\n",
    "====\n",
    "Train on (muon waveform x(t), simulated S21_sim(f)) → simulated binary counts.\n",
    "At test time, replace S21_sim(f) with measured S21_real_gain0(f) (VNA+Pitaya stitched)\n",
    "and predict the expected counts as if the muon passed through the real electronics.\n",
    "\n",
    "What you need to provide\n",
    "=========================\n",
    "- A dataset of simulated examples with:\n",
    "  * waveform: float32 array of shape [T] (uniformly sampled x(t))\n",
    "  * s21_sim: float32 array of shape [F] (amplitude in dB or linear, see CONFIG)\n",
    "  * target_count: float32 scalar (or int) — number of ones in the binary counter for that waveform.\n",
    "- One measured S21 curve for gain=0 (stitched VNA+Pitaya) of shape [F_meas].\n",
    "  It must be resampled to the same frequency grid as s21_sim (use the helper below).\n",
    "\n",
    "File/dir expectations (you can change paths):\n",
    "- data/sim_train.npz  with keys: 'waveforms' [N,T], 's21' [N,F], 'counts' [N]\n",
    "- data/sim_val.npz    with same keys for validation\n",
    "- data/s21_real_gain0.npy with shape [F]\n",
    "\n",
    "Key design choices\n",
    "==================\n",
    "- CNN1D encodes waveforms; MLP encodes S21; features are concatenated → MLP head predicts counts.\n",
    "- Optional frequency mask for f>40 MHz to attenuate unreliable regions in the S21 encoder input.\n",
    "- Strong normalizations: waveform standardization; S21 either dB normalized to peak=0 or linear gain.\n",
    "- Domain-robustness: light random perturbations of S21 during training (shift, tilt, noise) to avoid overfitting\n",
    "  to a single \"perfect\" simulated curve.\n",
    "\n",
    "Run\n",
    "===\n",
    "python sim2real_counter_predictor.py --train\n",
    "python sim2real_counter_predictor.py --predict-real\n",
    "\n",
    "This is a skeleton: adjust shapes, paths, and hyperparams to your data.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ------------------------------\n",
    "# Config\n",
    "# ------------------------------\n",
    "@dataclass\n",
    "class CONFIG:\n",
    "    # data paths\n",
    "    train_npz: str = \"data/sim_train.npz\"\n",
    "    val_npz: str = \"data/sim_val.npz\"\n",
    "    s21_real_path: str = \"data/s21_real_gain0.npy\"\n",
    "\n",
    "    # sampling and masking\n",
    "    freq_mask_cut_mhz: float = 40.0   # attenuate > this (MHz) in S21 encoder input\n",
    "    freq_grid_mhz: Optional[np.ndarray] = None  # if provided, used to build mask; else no mask\n",
    "    mask_attenuation: float = 0.4     # multiply S21 features above cut by this factor\n",
    "\n",
    "    # model\n",
    "    waveform_len: int = 2048\n",
    "    s21_len: int = 1024\n",
    "    wf_channels: int = 1\n",
    "\n",
    "    # training\n",
    "    batch_size: int = 64\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    epochs: int = 50\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # data augmentation on S21 (domain randomization light)\n",
    "    aug_shift_db: float = 0.5   # uniform shift in dB\n",
    "    aug_tilt_db: float = 0.8    # linear tilt across band (dB span)\n",
    "    aug_noise_db: float = 0.2   # additive gaussian noise (std, dB)\n",
    "\n",
    "    # normalization\n",
    "    s21_in_db: bool = True      # if True, inputs are in dB (recommend); else linear amplitude\n",
    "    s21_peak_to_0db: bool = True\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 1234):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Utilities\n",
    "# ------------------------------\n",
    "\n",
    "def resample_to_reference(x: np.ndarray, f_src: np.ndarray, f_ref: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Resample 1D array x defined on f_src to f_ref via linear interpolation.\n",
    "    Assumes both f_src and f_ref are sorted ascending.\n",
    "    \"\"\"\n",
    "    return np.interp(f_ref, f_src, x)\n",
    "\n",
    "\n",
    "def build_freq_mask_hz(freq_grid_hz: Optional[np.ndarray], low_hz: float, high_hz: float, attenuation: float) -> Optional[np.ndarray]:\n",
    "    if freq_grid_hz is None:\n",
    "        return None\n",
    "    m = np.ones_like(freq_grid_hz, dtype=np.float32)\n",
    "    m[(freq_grid_hz < low_hz) | (freq_grid_hz > high_hz)] = attenuation\n",
    "    return m\n",
    "\n",
    "\n",
    "def normalize_spectrum_db(spec_db: np.ndarray, peak_to_0db: bool = True) -> np.ndarray:\n",
    "    x = spec_db.astype(np.float32).copy()\n",
    "    if peak_to_0db:\n",
    "        peak = np.max(x)\n",
    "        x = x - peak\n",
    "    return x\n",
    "\n",
    "\n",
    "def augment_spec_db(x_db: np.ndarray, cfg: CONFIG) -> np.ndarray:\n",
    "    \"\"\"Apply simple domain randomization to spectra in dB: shift, tilt, and noise.\"\"\"\n",
    "    x = x_db.copy()\n",
    "    N = x.shape[-1]\n",
    "    # global shift\n",
    "    shift = np.random.uniform(-cfg.aug_shift_db, cfg.aug_shift_db)\n",
    "    x += shift\n",
    "    # linear tilt across band\n",
    "    tilt_span = np.random.uniform(-cfg.aug_tilt_db, cfg.aug_tilt_db)\n",
    "    lin = np.linspace(-0.5, 0.5, N, dtype=np.float32)\n",
    "    x += tilt_span * lin\n",
    "    # gaussian noise\n",
    "    noise = np.random.normal(0.0, cfg.aug_noise_db, size=N).astype(np.float32)\n",
    "    x += noise\n",
    "    return x\n",
    "\n",
    "\n",
    "def standardize_waveform(wf: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    m = wf.mean()\n",
    "    s = wf.std()\n",
    "    return ((wf - m) / (s + eps)).astype(np.float32)\n",
    "\n",
    "# ------------------------------\n",
    "# Dataset\n",
    "# ------------------------------\n",
    "class SimDataset(Dataset):\n",
    "    def __init__(self, npz_path: str, cfg: CONFIG, freq_mask: Optional[np.ndarray] = None, train: bool = True):\n",
    "        data = np.load(npz_path)\n",
    "        self.waveforms = data['waveforms'].astype(np.float32)  # [N,T]\n",
    "        self.s21 = data['s21'].astype(np.float32)              # [N,F]\n",
    "        self.counts = data['counts'].astype(np.float32)        # [N]\n",
    "        self.cfg = cfg\n",
    "        self.freq_mask = freq_mask.astype(np.float32) if freq_mask is not None else None\n",
    "        self.train = train\n",
    "        assert self.waveforms.shape[1] == cfg.waveform_len, \"Mismatch waveform_len\"\n",
    "        assert self.s21.shape[1] == cfg.s21_len, \"Mismatch s21_len\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.waveforms.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wf = self.waveforms[idx]\n",
    "        s21 = self.s21[idx]\n",
    "        y = self.counts[idx]\n",
    "        # normalize waveform\n",
    "        wf = standardize_waveform(wf)\n",
    "        # s21 in dB normalization\n",
    "        if self.cfg.s21_in_db:\n",
    "            s21 = normalize_s21_db(s21, peak_to_0db=self.cfg.s21_peak_to_0db)\n",
    "        # data augmentation on s21 during training\n",
    "        if self.train:\n",
    "            s21 = augment_s21_db(s21, self.cfg)\n",
    "        # apply frequency mask attenuation (feature-level downweighting)\n",
    "        if self.freq_mask is not None:\n",
    "            s21 = s21 * self.freq_mask\n",
    "        # torch tensors\n",
    "        wf = torch.from_numpy(wf)[None, :]  # [1,T]\n",
    "        s21 = torch.from_numpy(s21)         # [F]\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        return wf, s21, y\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Model\n",
    "# ------------------------------\n",
    "class WaveformEncoder(nn.Module):\n",
    "    def __init__(self, in_ch: int = 1, hidden: int = 64, t: int = 2048):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 32, kernel_size=9, stride=2, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 96, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(96, 128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "        )\n",
    "        self.proj = nn.Linear(128, hidden)\n",
    "\n",
    "    def forward(self, x):  # x: [B,1,T]\n",
    "        h = self.net(x)    # [B,128,1]\n",
    "        h = h.squeeze(-1)  # [B,128]\n",
    "        h = self.proj(h)   # [B,H]\n",
    "        return h\n",
    "\n",
    "\n",
    "class S21Encoder(nn.Module):\n",
    "    def __init__(self, f_len: int = 1024, hidden: int = 64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(f_len, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Linear(128, hidden)\n",
    "        )\n",
    "\n",
    "    def forward(self, s21):  # s21: [B,F]\n",
    "        return self.net(s21)\n",
    "\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, wf_hidden: int = 64, s_hidden: int = 64):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(wf_hidden + s_hidden, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, wf_feat, s_feat):\n",
    "        x = torch.cat([wf_feat, s_feat], dim=-1)\n",
    "        y = self.mlp(x).squeeze(-1)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Sim2RealModel(nn.Module):\n",
    "    def __init__(self, cfg: CONFIG):\n",
    "        super().__init__()\n",
    "        self.wf_enc = WaveformEncoder(in_ch=cfg.wf_channels, hidden=64, t=cfg.waveform_len)\n",
    "        self.s_enc = S21Encoder(f_len=cfg.s21_len, hidden=64)\n",
    "        self.head = Predictor(wf_hidden=64, s_hidden=64)\n",
    "\n",
    "    def forward(self, wf, s21):  # wf: [B,1,T]; s21: [B,F]\n",
    "        wf_feat = self.wf_enc(wf)\n",
    "        s_feat = self.s_enc(s21)\n",
    "        y = self.head(wf_feat, s_feat)\n",
    "        return y\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Training / Eval\n",
    "# ------------------------------\n",
    "\n",
    "def train(cfg: CONFIG):\n",
    "    set_seed()\n",
    "\n",
    "    # optional frequency mask (requires cfg.freq_grid_mhz)\n",
    "    freq_mask = build_freq_mask(cfg.freq_grid_mhz, cfg.freq_mask_cut_mhz, cfg.mask_attenuation)\n",
    "\n",
    "    ds_tr = SimDataset(cfg.train_npz, cfg, freq_mask=freq_mask, train=True)\n",
    "    ds_va = SimDataset(cfg.val_npz, cfg, freq_mask=freq_mask, train=False)\n",
    "\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=cfg.batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "    dl_va = DataLoader(ds_va, batch_size=cfg.batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    model = Sim2RealModel(cfg).to(cfg.device)\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, mode='min', factor=0.5, patience=5)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    best_val = float('inf')\n",
    "    best_path = Path(\"artifacts/best.pt\")\n",
    "    best_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        model.train()\n",
    "        tr_loss = 0.0\n",
    "        for wf, s21, y in dl_tr:\n",
    "            wf = wf.to(cfg.device)\n",
    "            s21 = s21.to(cfg.device)\n",
    "            y = y.to(cfg.device)\n",
    "            optim.zero_grad()\n",
    "            yhat = model(wf, s21)\n",
    "            loss = loss_fn(yhat, y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optim.step()\n",
    "            tr_loss += loss.item() * y.size(0)\n",
    "        tr_loss /= len(ds_tr)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        va_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for wf, s21, y in dl_va:\n",
    "                wf = wf.to(cfg.device)\n",
    "                s21 = s21.to(cfg.device)\n",
    "                y = y.to(cfg.device)\n",
    "                yhat = model(wf, s21)\n",
    "                loss = loss_fn(yhat, y)\n",
    "                va_loss += loss.item() * y.size(0)\n",
    "        va_loss /= len(ds_va)\n",
    "        sched.step(va_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | train {tr_loss:.4f} | val {va_loss:.4f}\")\n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            torch.save({\"model\": model.state_dict(), \"cfg\": cfg.__dict__}, best_path)\n",
    "            print(f\"  ↳ Saved best to {best_path} (val {best_val:.4f})\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_real(cfg: CONFIG):\n",
    "    # load best\n",
    "    ckpt = torch.load(\"artifacts/best.pt\", map_location=cfg.device)\n",
    "    model = Sim2RealModel(cfg).to(cfg.device)\n",
    "    model.load_state_dict(ckpt[\"model\"]) \n",
    "    model.eval()\n",
    "\n",
    "    # load data for prediction\n",
    "    real_s21 = np.load(cfg.s21_real_path).astype(np.float32)  # shape [F]\n",
    "    if cfg.s21_in_db:\n",
    "        real_s21 = normalize_s21_db(real_s21, peak_to_0db=cfg.s21_peak_to_0db)\n",
    "\n",
    "    # apply same frequency mask attenuation\n",
    "    freq_mask = build_freq_mask(cfg.freq_grid_mhz, cfg.freq_mask_cut_mhz, cfg.mask_attenuation)\n",
    "    if freq_mask is not None:\n",
    "        real_s21 = real_s21 * freq_mask\n",
    "\n",
    "    real_s21_t = torch.from_numpy(real_s21)[None, :].to(cfg.device)  # [1,F]\n",
    "\n",
    "    # Example: predict counts for each waveform in validation set with S21_real\n",
    "    data = np.load(cfg.val_npz)\n",
    "    waveforms = data['waveforms'].astype(np.float32)\n",
    "    wf_ds = []\n",
    "    for i in range(waveforms.shape[0]):\n",
    "        wf = standardize_waveform(waveforms[i])\n",
    "        wf_ds.append(torch.from_numpy(wf)[None, :])  # [1,T]\n",
    "    wf_batch = torch.stack(wf_ds, dim=0).to(cfg.device)  # [N,1,T]\n",
    "    s21_batch = real_s21_t.repeat(wf_batch.size(0), 1)   # [N,F]\n",
    "\n",
    "    yhat = model(wf_batch, s21_batch).cpu().numpy()      # predicted counts under real S21\n",
    "    out_path = Path(\"artifacts/pred_counts_real_gain0.npy\")\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    np.save(out_path, yhat)\n",
    "    print(f\"Saved predictions under real S21 to {out_path}\")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Main\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train\", action=\"store_true\", help=\"Train the model on simulated data\")\n",
    "    parser.add_argument(\"--predict-real\", action=\"store_true\", help=\"Predict counts for val set using real S21\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    cfg = CONFIG()\n",
    "\n",
    "    if args.train:\n",
    "        train(cfg)\n",
    "    if args.predict_real:\n",
    "        predict_with_real(cfg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
