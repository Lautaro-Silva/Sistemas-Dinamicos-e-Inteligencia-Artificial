{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0460a24-30d3-4515-b916-e62012713ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, WeightedRandomSampler, BatchSampler\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2cd8706-a40f-40ce-ba61-20679a859a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Señales de muones: torch.Size([10000, 1, 1000]), targets: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# CONFIG\n",
    "# -----------------------\n",
    "pulse_samples = 1000     # n_feat_muon\n",
    "n_feat_frec = 2500       # n_feat_frec\n",
    "batch_size = 512\n",
    "epochs = 120\n",
    "lr = 1e-3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_classes = 21\n",
    "\n",
    "# Paths a tus datasets\n",
    "muon_dataset_path = \"muon_dataset_segmented_v3.npz\"\n",
    "simulated_npz_path = \"Espectros/spectrum_simulated.npz\"\n",
    "real_npz_path = \"Espectros/spectrum_real.npz\"\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# CARGA DE DATASET DE MUONES\n",
    "# -----------------------\n",
    "data = np.load(muon_dataset_path)\n",
    "X_muones = data['X']        # shape (num_pulsos, pulse_samples)\n",
    "y_muones = data['y']        # shape (num_pulsos,)\n",
    "\n",
    "# -----------------------\n",
    "# Normalizar pulsos de muones al mismo rango\n",
    "# -----------------------\n",
    "# Tomar el máximo global de todos los pulsos\n",
    "max_global = X_muones.max()\n",
    "\n",
    "# Escalar todos los pulsos para que el pulso con mayor valor llegue a 1\n",
    "X_muones = X_muones / (max_global + 1e-8)  # evitar división por cero\n",
    "\n",
    "# Convertir a tensores PyTorch\n",
    "X_muones = torch.tensor(X_muones, dtype=torch.float32).unsqueeze(1)  # (N,1,pulse_samples)\n",
    "y_muones = torch.tensor(y_muones, dtype=torch.long).squeeze()         # (N,)\n",
    "\n",
    "print(f\"Señales de muones: {X_muones.shape}, targets: {y_muones.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# CARGA DE ESPECTROS\n",
    "# -----------------------\n",
    "def load_npz(path):\n",
    "    data = np.load(path)\n",
    "    freq = data['freq']\n",
    "    att = data['attenuation_db']\n",
    "    return torch.tensor(freq, dtype=torch.float32), torch.tensor(att, dtype=torch.float32)\n",
    "\n",
    "sim_freq, H_sim = load_npz(simulated_npz_path)\n",
    "real_freq, H_real = load_npz(real_npz_path)\n",
    "\n",
    "# -----------------------\n",
    "# Normalizar espectros simulados H_sim de 0 a 1\n",
    "# -----------------------\n",
    "H_sim_min = H_sim.min()\n",
    "H_sim_max = H_sim.max()\n",
    "H_sim_norm = (H_sim - H_sim_min) / (H_sim_max - H_sim_min + 1e-8)  # evitar división por cero\n",
    "\n",
    "# Repetir para cada ejemplo\n",
    "H_sim_batch = H_sim_norm.repeat(X_muones.shape[0], 1)  # (N, n_feat_frec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6429a732-4d92-4983-b7ee-9df488b5480c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TwoBranchNet                             [1, 21]                   --\n",
       "├─Sequential: 1-1                        [1, 512]                  --\n",
       "│    └─Conv1d: 2-1                       [1, 64, 1000]             512\n",
       "│    └─BatchNorm1d: 2-2                  [1, 64, 1000]             128\n",
       "│    └─LeakyReLU: 2-3                    [1, 64, 1000]             --\n",
       "│    └─MaxPool1d: 2-4                    [1, 64, 500]              --\n",
       "│    └─Conv1d: 2-5                       [1, 128, 500]             41,088\n",
       "│    └─BatchNorm1d: 2-6                  [1, 128, 500]             256\n",
       "│    └─LeakyReLU: 2-7                    [1, 128, 500]             --\n",
       "│    └─MaxPool1d: 2-8                    [1, 128, 250]             --\n",
       "│    └─Conv1d: 2-9                       [1, 256, 250]             164,096\n",
       "│    └─BatchNorm1d: 2-10                 [1, 256, 250]             512\n",
       "│    └─LeakyReLU: 2-11                   [1, 256, 250]             --\n",
       "│    └─MaxPool1d: 2-12                   [1, 256, 125]             --\n",
       "│    └─Conv1d: 2-13                      [1, 512, 125]             393,728\n",
       "│    └─BatchNorm1d: 2-14                 [1, 512, 125]             1,024\n",
       "│    └─LeakyReLU: 2-15                   [1, 512, 125]             --\n",
       "│    └─AdaptiveAvgPool1d: 2-16           [1, 512, 1]               --\n",
       "│    └─Flatten: 2-17                     [1, 512]                  --\n",
       "├─Sequential: 1-2                        [1, 32]                   --\n",
       "│    └─Linear: 2-18                      [1, 128]                  320,128\n",
       "│    └─LeakyReLU: 2-19                   [1, 128]                  --\n",
       "│    └─Linear: 2-20                      [1, 64]                   8,256\n",
       "│    └─LeakyReLU: 2-21                   [1, 64]                   --\n",
       "│    └─Linear: 2-22                      [1, 32]                   2,080\n",
       "│    └─LeakyReLU: 2-23                   [1, 32]                   --\n",
       "├─Sequential: 1-3                        [1, 21]                   --\n",
       "│    └─Linear: 2-24                      [1, 256]                  139,520\n",
       "│    └─LeakyReLU: 2-25                   [1, 256]                  --\n",
       "│    └─Linear: 2-26                      [1, 128]                  32,896\n",
       "│    └─LeakyReLU: 2-27                   [1, 128]                  --\n",
       "│    └─Dropout: 2-28                     [1, 128]                  --\n",
       "│    └─Linear: 2-29                      [1, 21]                   2,709\n",
       "==========================================================================================\n",
       "Total params: 1,106,933\n",
       "Trainable params: 1,106,933\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 111.80\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 4.10\n",
       "Params size (MB): 4.43\n",
       "Estimated Total Size (MB): 8.54\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------\n",
    "# DEFINICIÓN DE LA RED\n",
    "# -----------------------\n",
    "class TwoBranchNet(nn.Module):\n",
    "    def __init__(self, n_feat_muon=1000, n_feat_frec=2500, n_classes=21):\n",
    "        super(TwoBranchNet, self).__init__()\n",
    "        \n",
    "        # --- Rama CNN Muones (Versión modificada) ---\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            # Capa de convolución inicial con kernel más pequeño y más filtros\n",
    "            nn.Conv1d(1, 64, kernel_size=7, padding='same'),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool1d(2),\n",
    "\n",
    "            # Capa adicional para mayor profundidad\n",
    "            nn.Conv1d(64, 128, kernel_size=5, padding='same'),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool1d(2),\n",
    "    \n",
    "            nn.Conv1d(128, 256, kernel_size=5, padding='same'),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool1d(2),\n",
    "\n",
    "            # Capa final de convolución para aprender características más complejas\n",
    "            nn.Conv1d(256, 512, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "    \n",
    "            # Reducir dimensionalidad global con AvgPool para no perder información\n",
    "            nn.AdaptiveAvgPool1d(1), # (batch, 512, 1)\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Rama 2: Respuesta en frecuencia simulada (MLP)\n",
    "        self.mlp_branch = nn.Sequential(\n",
    "            nn.Linear(n_feat_frec, 128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "        # --- Cabezal de fusión ---\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512 + 32, 256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_muon, x_freq):\n",
    "        v1 = self.cnn_branch(x_muon)\n",
    "        v1 = v1.view(v1.size(0), -1)\n",
    "        v2 = self.mlp_branch(x_freq)\n",
    "        merged = torch.cat([v1, v2], dim=1)\n",
    "        out = self.head(merged)\n",
    "        return out\n",
    "\n",
    "# -----------------------\n",
    "# Inicializar modelo\n",
    "# -----------------------\n",
    "model = TwoBranchNet(n_feat_muon=pulse_samples, n_feat_frec=n_feat_frec, n_classes=n_classes).to(device)\n",
    "summary(model, input_size=[(1,1,1000), (1,2500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6ae380-d692-4784-8c85-af792c41c208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de clases:\n",
      "Clase 0: 22 muestras\n",
      "Clase 1: 11 muestras\n",
      "Clase 2: 29 muestras\n",
      "Clase 3: 90 muestras\n",
      "Clase 4: 291 muestras\n",
      "Clase 5: 1006 muestras\n",
      "Clase 6: 2193 muestras\n",
      "Clase 7: 2321 muestras\n",
      "Clase 8: 37 muestras\n",
      "Train size: 6000, Val size: 2000, Test size: 2000\n",
      "\n",
      "Verificando distribución en 3 batches:\n",
      "Batch 0: [7, 4, 6, 7, 25, 103, 160, 194, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Clase 0: 7 muestras\n",
      "  Clase 1: 4 muestras\n",
      "  Clase 2: 6 muestras\n",
      "  Clase 3: 7 muestras\n",
      "  Clase 4: 25 muestras\n",
      "  Clase 5: 103 muestras\n",
      "  Clase 6: 160 muestras\n",
      "  Clase 7: 194 muestras\n",
      "  Clase 8: 6 muestras\n",
      "Batch 1: [5, 4, 6, 8, 25, 82, 197, 181, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Clase 0: 5 muestras\n",
      "  Clase 1: 4 muestras\n",
      "  Clase 2: 6 muestras\n",
      "  Clase 3: 8 muestras\n",
      "  Clase 4: 25 muestras\n",
      "  Clase 5: 82 muestras\n",
      "  Clase 6: 197 muestras\n",
      "  Clase 7: 181 muestras\n",
      "  Clase 8: 4 muestras\n",
      "Batch 2: [6, 3, 7, 7, 26, 98, 179, 181, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Clase 0: 6 muestras\n",
      "  Clase 1: 3 muestras\n",
      "  Clase 2: 7 muestras\n",
      "  Clase 3: 7 muestras\n",
      "  Clase 4: 26 muestras\n",
      "  Clase 5: 98 muestras\n",
      "  Clase 6: 179 muestras\n",
      "  Clase 7: 181 muestras\n",
      "  Clase 8: 5 muestras\n",
      "\n",
      "Pesos de clase (suavizados) calculados: [0.0608239620923996, 2.5496227741241455, 1.347018837928772, 0.8701941967010498, 0.5936309695243835, 0.43674513697624207, 0.36109688878059387, 0.3519996702671051, 0.6989803314208984, 0.0608239620923996, 0.0608239620923996, 0.0608239620923996, 0.0608239620923996, 0.0608239620923996, 0.0608239620923996, 0.0608239620923996, 0.0608239620923996, 0.0608239620923996, 0.0608239620923996, 0.0608239620923996, 0.0608239620923996]\n",
      "Epoch 1/120 | Train Loss: 38.2110, Acc: 0.4710 | Val Loss: 1.9923, Acc: 0.3905\n",
      "Epoch 2/120 | Train Loss: 33.6966, Acc: 0.5868 | Val Loss: 2.3060, Acc: 0.1775\n",
      "Epoch 3/120 | Train Loss: 31.5654, Acc: 0.6290 | Val Loss: 1.8746, Acc: 0.2870\n",
      "Epoch 4/120 | Train Loss: 31.2529, Acc: 0.6183 | Val Loss: 1.3520, Acc: 0.6385\n",
      "Epoch 5/120 | Train Loss: 31.1156, Acc: 0.6433 | Val Loss: 1.1986, Acc: 0.6890\n",
      "Epoch 6/120 | Train Loss: 30.9218, Acc: 0.6535 | Val Loss: 1.0403, Acc: 0.6970\n",
      "Epoch 7/120 | Train Loss: 30.7784, Acc: 0.6480 | Val Loss: 0.9837, Acc: 0.6950\n",
      "Epoch 8/120 | Train Loss: 31.6654, Acc: 0.6668 | Val Loss: 0.9488, Acc: 0.7055\n",
      "Epoch 9/120 | Train Loss: 31.6483, Acc: 0.6503 | Val Loss: 0.8862, Acc: 0.7295\n",
      "Epoch 10/120 | Train Loss: 31.1368, Acc: 0.6597 | Val Loss: 1.2841, Acc: 0.5185\n",
      "Epoch 11/120 | Train Loss: 31.2300, Acc: 0.6830 | Val Loss: 1.1467, Acc: 0.6110\n",
      "Epoch 12/120 | Train Loss: 31.6058, Acc: 0.6843 | Val Loss: 0.9099, Acc: 0.6995\n",
      "Epoch 13/120 | Train Loss: 31.3336, Acc: 0.6775 | Val Loss: 0.9306, Acc: 0.7075\n",
      "Epoch 14/120 | Train Loss: 31.5767, Acc: 0.6928 | Val Loss: 1.0325, Acc: 0.6525\n",
      "Epoch 15/120 | Train Loss: 31.1395, Acc: 0.6870 | Val Loss: 1.0831, Acc: 0.6215\n",
      "Epoch 16/120 | Train Loss: 31.5619, Acc: 0.6890 | Val Loss: 1.1909, Acc: 0.6160\n",
      "Epoch 17/120 | Train Loss: 31.3446, Acc: 0.6983 | Val Loss: 0.9056, Acc: 0.7330\n",
      "Epoch 18/120 | Train Loss: 31.0427, Acc: 0.7065 | Val Loss: 1.0145, Acc: 0.6790\n",
      "Epoch 19/120 | Train Loss: 31.2824, Acc: 0.6908 | Val Loss: 1.7293, Acc: 0.4575\n",
      "Epoch 20/120 | Train Loss: 30.9769, Acc: 0.7057 | Val Loss: 2.1072, Acc: 0.4150\n",
      "Epoch 21/120 | Train Loss: 30.4415, Acc: 0.6927 | Val Loss: 1.9156, Acc: 0.4195\n",
      "Epoch 22/120 | Train Loss: 31.2757, Acc: 0.6988 | Val Loss: 0.8491, Acc: 0.7285\n"
     ]
    }
   ],
   "source": [
    "# Iniciar el cronómetro\n",
    "start_time = time.time()\n",
    "\n",
    "# -----------------------\n",
    "# Datasets y DataLoaders\n",
    "# -----------------------\n",
    "\n",
    "dataset = TensorDataset(X_muones, H_sim_batch, y_muones)\n",
    "\n",
    "# Primer split: Train+Val vs Test (80/20)\n",
    "n_total = len(dataset)\n",
    "n_trainval = int(0.8 * n_total)\n",
    "n_test = n_total - n_trainval\n",
    "trainval_dataset, test_dataset = random_split(dataset, [n_trainval, n_test])\n",
    "\n",
    "# Segundo split: Train vs Val (70/30 del 80%)\n",
    "n_train = int(0.75 * n_trainval)\n",
    "n_val  = n_trainval - n_train\n",
    "train_dataset, val_dataset = random_split(trainval_dataset, [n_train, n_val])\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Solución usando BatchSampler\n",
    "# -----------------------\n",
    "\n",
    "class MinimumClassBatchSampler:\n",
    "    def __init__(self, dataset, n_classes, batch_size, min_samples_per_class=2):\n",
    "        self.dataset = dataset\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.min_samples_per_class = min_samples_per_class\n",
    "        \n",
    "        # Extraer targets del dataset\n",
    "        self.targets = self._extract_targets(dataset)\n",
    "        \n",
    "        # Obtener índices por clase\n",
    "        self.class_indices = [[] for _ in range(n_classes)]\n",
    "        for idx in range(len(dataset)):\n",
    "            class_id = self.targets[idx]\n",
    "            if 0 <= class_id < n_classes:\n",
    "                self.class_indices[class_id].append(idx)\n",
    "        \n",
    "        # Verificar distribución\n",
    "        print(\"Distribución de clases:\")\n",
    "        for class_id, indices in enumerate(self.class_indices):\n",
    "            if indices:\n",
    "                print(f\"Clase {class_id}: {len(indices)} muestras\")\n",
    "    \n",
    "    def _extract_targets(self, dataset):\n",
    "        \"\"\"Extrae los targets del dataset\"\"\"\n",
    "        targets = []\n",
    "        for i in range(len(dataset)):\n",
    "            if isinstance(dataset, torch.utils.data.Subset):\n",
    "                original_idx = dataset.indices[i]\n",
    "                _, _, target = dataset.dataset[original_idx]\n",
    "            else:\n",
    "                _, _, target = dataset[i]\n",
    "            \n",
    "            class_id = target.item() if torch.is_tensor(target) else target\n",
    "            targets.append(class_id)\n",
    "        return targets\n",
    "    \n",
    "    def __iter__(self):\n",
    "        num_batches = len(self.dataset) // self.batch_size\n",
    "        \n",
    "        for _ in range(num_batches):\n",
    "            batch_indices = []\n",
    "            \n",
    "            # Para cada clase que tenga muestras, tomar al menos min_samples_per_class\n",
    "            for class_id in range(self.n_classes):\n",
    "                if self.class_indices[class_id]:\n",
    "                    n_to_take = min(self.min_samples_per_class, len(self.class_indices[class_id]))\n",
    "                    selected = random.sample(self.class_indices[class_id], n_to_take)\n",
    "                    batch_indices.extend(selected)\n",
    "            \n",
    "            # Si nos pasamos del batch_size, recortar\n",
    "            if len(batch_indices) > self.batch_size:\n",
    "                batch_indices = random.sample(batch_indices, self.batch_size)\n",
    "            # Si nos falta, completar con muestras aleatorias\n",
    "            elif len(batch_indices) < self.batch_size:\n",
    "                remaining = self.batch_size - len(batch_indices)\n",
    "                all_indices = list(range(len(self.dataset)))\n",
    "                additional = random.sample(all_indices, remaining)\n",
    "                batch_indices.extend(additional)\n",
    "            \n",
    "            yield batch_indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size\n",
    "\n",
    "# -----------------------\n",
    "# Configuración CORREGIDA\n",
    "# -----------------------\n",
    "\n",
    "# Crear batch sampler\n",
    "batch_sampler = MinimumClassBatchSampler(\n",
    "    train_dataset, \n",
    "    n_classes=n_classes, \n",
    "    batch_size=batch_size, \n",
    "    min_samples_per_class=3\n",
    ")\n",
    "\n",
    "# Configurar DataLoader sin sampler, usando batch_sampler en su lugar\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_sampler=batch_sampler,  # Usar batch_sampler en lugar de sampler\n",
    ")\n",
    "\n",
    "# Val y Test se mantienen igual\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}, Test size: {len(test_dataset)}\")\n",
    "\n",
    "# -----------------------\n",
    "# Función de verificación CORREGIDA\n",
    "# -----------------------\n",
    "\n",
    "def check_batch_distribution(loader, n_classes, num_batches=3):\n",
    "    \"\"\"Verifica la distribución de clases en los primeros batches\"\"\"\n",
    "    print(f\"\\nVerificando distribución en {num_batches} batches:\")\n",
    "    \n",
    "    for batch_idx, (x_muon, x_freq, y) in enumerate(loader):\n",
    "        if batch_idx >= num_batches:\n",
    "            break\n",
    "        \n",
    "        class_counts = torch.bincount(y, minlength=n_classes)\n",
    "        print(f\"Batch {batch_idx}: {class_counts.tolist()}\")\n",
    "        \n",
    "        # Verificar clases con muestras\n",
    "        for class_id, count in enumerate(class_counts):\n",
    "            if count > 0:\n",
    "                print(f\"  Clase {class_id}: {count} muestras\")\n",
    "\n",
    "# Verificar\n",
    "check_batch_distribution(train_loader, n_classes, num_batches=3)\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Inicialización de modelo, optimizer y scheduler\n",
    "# -----------------------\n",
    "model = TwoBranchNet(n_feat_muon=pulse_samples, n_feat_frec=n_feat_frec, n_classes=n_classes).to(device)\n",
    "\n",
    "# Añadir regularización L2 (weight decay) al optimizador\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-7)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Nueva función de pérdida combinada\n",
    "# -----------------------\n",
    "def custom_combined_loss(outputs, targets, class_weights=None, min_fraction=0.01, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Combina la Entropía Cruzada Ponderada con una penalización\n",
    "    por la baja fracción de predicciones para clases presentes en el lote.\n",
    "    \"\"\"\n",
    "    ce_loss = nn.CrossEntropyLoss(weight=class_weights)(outputs, targets)\n",
    "    \n",
    "    if alpha <= 0:\n",
    "        return ce_loss\n",
    "\n",
    "    unique_classes_in_batch = torch.unique(targets)\n",
    "    penalty = 0.0\n",
    "    total_unique_classes = 0\n",
    "    probs = F.softmax(outputs, dim=1)\n",
    "    \n",
    "    for c in unique_classes_in_batch:\n",
    "        predicted_fraction = (probs.argmax(dim=1) == c).float().mean()\n",
    "        penalty += torch.relu(min_fraction - predicted_fraction)\n",
    "        total_unique_classes += 1\n",
    "\n",
    "    if total_unique_classes > 0:\n",
    "        penalty /= total_unique_classes\n",
    "\n",
    "    total_loss = ce_loss + alpha * penalty\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n",
    "# Paso 1: Calcular los pesos de clase de forma robusta\n",
    "y_train_indices = train_dataset.indices\n",
    "y_train_labels = y_muones[y_train_indices].cpu().numpy()\n",
    "\n",
    "# Contar las ocurrencias de cada clase presente\n",
    "unique_classes, counts = np.unique(y_train_labels, return_counts=True)\n",
    "\n",
    "# Crear un mapa de conteo para acceso rápido\n",
    "class_counts = {cls: count for cls, count in zip(unique_classes, counts)}\n",
    "\n",
    "# Crear el tensor de pesos completo (21 elementos)\n",
    "class_weights = torch.zeros(n_classes, dtype=torch.float32).to(device)\n",
    "\n",
    "# Calcular los pesos de clase con una función de suavizado logarítmica\n",
    "for cls in unique_classes:\n",
    "    weight = 1.0 / np.log10(1 + class_counts[cls])\n",
    "    class_weights[cls] = weight\n",
    "\n",
    "# Asignar un peso mínimo a las clases que tienen peso cero\n",
    "min_weight = 0.05  # Puedes ajustar este valor\n",
    "for i in range(n_classes):\n",
    "    if class_weights[i] == 0:\n",
    "        class_weights[i] = min_weight\n",
    "\n",
    "# Normalizar los pesos para que sumen el número de clases\n",
    "class_weights = class_weights / class_weights.sum() * len(unique_classes)\n",
    "# ---------------------------\n",
    "\n",
    "print(\"\\nPesos de clase (suavizados) calculados:\", class_weights.tolist())\n",
    "\n",
    "\n",
    "# Hiperparámetros de la pérdida combinada\n",
    "alpha_value = 500  # Ajusta este valor para controlar la penalización\n",
    "min_fraction_value = 0.1 # Puedes ajustar este valor también\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss, running_corrects = 0.0, 0\n",
    "    for x_muon_batch, x_freq_batch, y_batch in train_loader:\n",
    "        x_muon_batch = x_muon_batch.to(device)\n",
    "        x_freq_batch = x_freq_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_muon_batch, x_freq_batch)\n",
    "        \n",
    "        # Usar la nueva función de pérdida\n",
    "        loss = custom_combined_loss(\n",
    "            output, y_batch,\n",
    "            class_weights=class_weights,\n",
    "            min_fraction=min_fraction_value,\n",
    "            alpha=alpha_value\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * x_muon_batch.size(0)\n",
    "        preds = output.argmax(dim=1)\n",
    "        running_corrects += (preds == y_batch).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / n_train\n",
    "    epoch_acc  = running_corrects / n_train\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accs.append(epoch_acc)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_running_loss, val_running_corrects = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x_muon_batch, x_freq_batch, y_batch in val_loader:\n",
    "            x_muon_batch = x_muon_batch.to(device)\n",
    "            x_freq_batch = x_freq_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            val_output = model(x_muon_batch, x_freq_batch)\n",
    "            val_loss = custom_combined_loss(\n",
    "                val_output, y_batch,\n",
    "                class_weights=class_weights,\n",
    "                min_fraction=min_fraction_value,\n",
    "                alpha=0 # No penalizar en validación, solo evaluar la pérdida real\n",
    "            )\n",
    "\n",
    "            val_running_loss += val_loss.item() * x_muon_batch.size(0)\n",
    "            preds = val_output.argmax(dim=1)\n",
    "            val_running_corrects += (preds == y_batch).sum().item()\n",
    "\n",
    "    val_epoch_loss = val_running_loss / n_val\n",
    "    val_epoch_acc  = val_running_corrects / n_val\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accs.append(val_epoch_acc)\n",
    "\n",
    "    scheduler.step(val_epoch_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "          f\"Train Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_epoch_loss:.4f}, Acc: {val_epoch_acc:.4f}\")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Guardar pesos\n",
    "# -----------------------\n",
    "os.makedirs(\"model_weights\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"model_weights/two_branch_net.pth\")\n",
    "print(\"Pesos guardados en 'model_weights/two_branch_net.pth'\")\n",
    "\n",
    "# -----------------------\n",
    "# Evaluación y métricas\n",
    "# -----------------------\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_muon_batch, x_freq_batch, y_batch in test_loader:\n",
    "        x_muon_batch = x_muon_batch.to(device)\n",
    "        x_freq_batch = x_freq_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        output = model(x_muon_batch, x_freq_batch)\n",
    "        preds = output.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=list(range(n_classes)))\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Etiqueta real\")\n",
    "plt.title(\"Matriz de Confusión\")\n",
    "plt.show()\n",
    "\n",
    "# Reporte de clasificación\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n",
    "\n",
    "# -----------------------\n",
    "# Plot de pérdida y accuracy\n",
    "# -----------------------\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"CrossEntropy Loss\")\n",
    "plt.title(\"Pérdida\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_accs, label=\"Train Acc\")\n",
    "plt.plot(val_accs, label=\"Val Acc\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Precisión\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Tiempo transcurrido hasta este punto\n",
    "mid_time = time.time()\n",
    "print(f\"\\nTiempo de simulacion: {mid_time - start_time:.4f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f0a97-1baf-4641-9de1-e951764294d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Evaluación en Test\n",
    "# -----------------------\n",
    "model.eval()\n",
    "test_preds_all = []\n",
    "test_targets_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_muon_batch, x_freq_batch, y_batch in test_loader:\n",
    "        x_muon_batch = x_muon_batch.to(device)\n",
    "        x_freq_batch = x_freq_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        test_output = model(x_muon_batch, x_freq_batch)\n",
    "        preds = test_output.argmax(dim=1)   # <- argmax para clases\n",
    "        test_preds_all.append(preds.cpu())\n",
    "        test_targets_all.append(y_batch.cpu())\n",
    "\n",
    "# Concatenar todos los batches\n",
    "test_preds_all = torch.cat(test_preds_all)\n",
    "test_targets_all = torch.cat(test_targets_all)\n",
    "\n",
    "# Accuracy\n",
    "acc_test = accuracy_score(test_targets_all.numpy(), test_preds_all.numpy())\n",
    "print(f\"Accuracy en Test: {acc_test*100:.2f}%\")\n",
    "\n",
    "# Histograma de clases predichas vs reales\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(test_targets_all.numpy(), bins=np.arange(-0.5, 21.5, 1), alpha=0.6, label=\"Target\", edgecolor='k')\n",
    "plt.hist(test_preds_all.numpy(), bins=np.arange(-0.5, 21.5, 1), alpha=0.6, label=\"Predicción\", edgecolor='k')\n",
    "plt.xlabel(\"Clase\")\n",
    "plt.ylabel(\"Cantidad\")\n",
    "plt.title(\"Distribución de clases en Test\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Matriz de confusión\n",
    "plt.subplot(1,2,2)\n",
    "cm = confusion_matrix(test_targets_all.numpy(), test_preds_all.numpy(), labels=np.arange(21))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(21))\n",
    "disp.plot(ax=plt.gca(), cmap='Blues', colorbar=False)\n",
    "plt.title(\"Matriz de Confusión\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d7a52-5a7a-4b65-8baa-1bf4186203ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13",
   "language": "python",
   "name": "python313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
