{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d18cd0-d1ca-44ea-b285-fc40542bb872",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple Jupyter-friendly Sim→Real counter predictor (minimal, opinionated)\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "Objetivo\n",
    "========\n",
    "Entrenar con (waveform_muon, gain0_sim) -> counts (clase 0..20).\n",
    "En test reemplazar gain0_sim por combined_real_gain0 y predecir los counts esperados.\n",
    "\n",
    "Principios de diseño (simplificados a pedido):\n",
    "- Espectros en dB, normalizados a pico = 0 dB (siempre).\n",
    "- Enmascaramiento DURO fuera de [1e3, 1e9] Hz (se multiplican por 0).\n",
    "- Sin augmentations ni randomizaciones.\n",
    "- No hay resampling en el código: **asumo que gain0_sim y combined_real_gain0 ya están en la MISMA grilla**\n",
    "  y que esa grilla está guardada en `data/freq_grid_hz.npy` (Hz). Si no la tenés, creala\n",
    "  con la misma longitud que tus espectros. Esto evita complejidad en el script.\n",
    "- Salida: clasificación con 21 clases (0..20).\n",
    "- Código pensado para ejecutarse en un notebook: funciones que devuelven historiales y permiten\n",
    "  graficar losses, comparar distribuciones y visualizar la matriz de confusión.\n",
    "\n",
    "Archivos esperados (poner en data/):\n",
    "- sim_train.npz  -> keys: 'waveforms' (N,T), 'gain0_sim' (N,F, dB), 'counts' (N) integers 0..20\n",
    "- sim_val.npz    -> same structure (validación)\n",
    "- combined_real_gain0.npy -> shape (F,) en dB (misma grilla que gain0_sim)\n",
    "- freq_grid_hz.npy -> shape (F,) con frecuencias en Hz (misma malla para los espectros)\n",
    "\n",
    "Salida producida:\n",
    "- artifacts/best_simple.pt                 (modelo guardado)\n",
    "- artifacts/pred_counts_real_gain0.npy     (predicciones sobre val usando combined_real_gain0)\n",
    "- functions devuelven arrays para plotear en notebook\n",
    "\n",
    "Instrucciones de uso (notebook):\n",
    "1) Cargá este script como módulo o pegalo en una celda.\n",
    "2) Llamá a `train_model(...)` para entrenar y recibir `history`.\n",
    "3) Usá `plot_losses(history)` y `plot_confusion(y_true, y_pred)` para visualizar.\n",
    "4) Ejecutá `predict_with_real(...)` para obtener predicciones usando `combined_real_gain0`.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "# -----------------------------\n",
    "# Minimal user-editable variables\n",
    "# -----------------------------\n",
    "TRAIN_NPZ = 'data/sim_train.npz'\n",
    "VAL_NPZ = 'data/sim_val.npz'\n",
    "REAL_SPEC_NPY = 'data/combined_real_gain0.npy'\n",
    "FREQ_GRID_NPY = 'data/freq_grid_hz.npy'   # required: same length as spec\n",
    "ARTIFACTS_DIR = Path('artifacts')\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "\n",
    "# Frequency bounds (hard mask)\n",
    "LOW_HZ = 1e3\n",
    "HIGH_HZ = 1e9\n",
    "\n",
    "# Fixed problem dimensions (will be inferred from data on load)\n",
    "NUM_CLASSES = 21  # classes 0..20\n",
    "\n",
    "# -----------------------------\n",
    "# Utility functions\n",
    "# -----------------------------\n",
    "\n",
    "def load_freq_grid():\n",
    "    p = Path(FREQ_GRID_NPY)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Freq grid file not found: {p}. Create a freq_grid_hz.npy matching your spectra\")\n",
    "    return np.load(p)\n",
    "\n",
    "\n",
    "def build_hard_mask(freq_grid_hz: np.ndarray, low_hz: float = LOW_HZ, high_hz: float = HIGH_HZ) -> np.ndarray:\n",
    "    mask = np.ones_like(freq_grid_hz, dtype=np.float32)\n",
    "    mask[(freq_grid_hz < low_hz) | (freq_grid_hz > high_hz)] = 0.0\n",
    "    return mask\n",
    "\n",
    "\n",
    "def normalize_spec_db_to_peak(spec_db: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Subtract max so peak is 0 dB. Works on 1D arrays or batch arrays (last axis = freq).\"\"\"\n",
    "    spec_db = np.array(spec_db, dtype=np.float32)\n",
    "    if spec_db.ndim == 1:\n",
    "        return spec_db - np.max(spec_db)\n",
    "    else:\n",
    "        return spec_db - np.max(spec_db, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "def standardize_waveform(wf: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    wf = np.array(wf, dtype=np.float32)\n",
    "    m = wf.mean()\n",
    "    s = wf.std()\n",
    "    return (wf - m) / (s + eps)\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset\n",
    "# -----------------------------\n",
    "class SimpleSimDataset(Dataset):\n",
    "    def __init__(self, npz_path: str, freq_mask: np.ndarray):\n",
    "        data = np.load(npz_path)\n",
    "        self.waveforms = data['waveforms'].astype(np.float32)  # [N,T]\n",
    "        self.spec = data['gain0_sim'].astype(np.float32)      # [N,F] in dB\n",
    "        self.counts = data['counts'].astype(np.int64)         # [N]\n",
    "        assert self.waveforms.ndim == 2\n",
    "        assert self.spec.ndim == 2\n",
    "        assert self.waveforms.shape[0] == self.spec.shape[0] == self.counts.shape[0]\n",
    "        self.freq_mask = freq_mask.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.waveforms.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wf = standardize_waveform(self.waveforms[idx])  # [T]\n",
    "        spec = normalize_spec_db_to_peak(self.spec[idx]) # [F]\n",
    "        spec = spec * self.freq_mask\n",
    "        y = int(self.counts[idx])\n",
    "        if y < 0:\n",
    "            y = 0\n",
    "        if y >= NUM_CLASSES:\n",
    "            y = NUM_CLASSES - 1\n",
    "        wf_t = torch.from_numpy(wf)[None, :].float()   # [1,T]\n",
    "        spec_t = torch.from_numpy(spec).float()        # [F]\n",
    "        return wf_t, spec_t, torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# -----------------------------\n",
    "# Model (small, simple)\n",
    "# -----------------------------\n",
    "class WaveformEncoder(nn.Module):\n",
    "    def __init__(self, in_ch: int = 1, out_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 32, kernel_size=9, stride=2, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, out_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SpecEncoder(nn.Module):\n",
    "    def __init__(self, f_len: int, out_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(f_len, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, out_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, spec_len: int, wf_out: int = 64, spec_out: int = 64, n_classes: int = NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        self.wf_enc = WaveformEncoder(in_ch=1, out_dim=wf_out)\n",
    "        self.spec_enc = SpecEncoder(f_len=spec_len, out_dim=spec_out)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(wf_out + spec_out, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, wf, spec):\n",
    "        # wf: [B,1,T], spec: [B,F]\n",
    "        wf_feat = self.wf_enc(wf)\n",
    "        spec_feat = self.spec_enc(spec)\n",
    "        x = torch.cat([wf_feat, spec_feat], dim=-1)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "# -----------------------------\n",
    "# Training / utilities for notebook\n",
    "# -----------------------------\n",
    "\n",
    "def train_model(train_npz: str = TRAIN_NPZ, val_npz: str = VAL_NPZ, epochs: int = EPOCHS, batch_size: int = BATCH_SIZE, lr: float = LR) -> Dict[str, Any]:\n",
    "    # load freq grid and build mask\n",
    "    freq_grid = load_freq_grid()\n",
    "    mask = build_hard_mask(freq_grid, LOW_HZ, HIGH_HZ)\n",
    "\n",
    "    # datasets and loaders\n",
    "    ds_tr = SimpleSimDataset(train_npz, mask)\n",
    "    ds_va = SimpleSimDataset(val_npz, mask)\n",
    "    spec_len = ds_tr.spec.shape[1]\n",
    "\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = SimpleClassifier(spec_len=spec_len).to(DEVICE)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    best_val = float('inf')\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for wf, spec, y in dl_tr:\n",
    "            wf = wf.to(DEVICE)\n",
    "            spec = spec.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            logits = model(wf, spec)\n",
    "            loss = loss_fn(logits, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            running_loss += loss.item() * y.size(0)\n",
    "        train_loss = running_loss / len(ds_tr)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        running_val = 0.0\n",
    "        with torch.no_grad():\n",
    "            for wf, spec, y in dl_va:\n",
    "                wf = wf.to(DEVICE)\n",
    "                spec = spec.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "                logits = model(wf, spec)\n",
    "                loss = loss_fn(logits, y)\n",
    "                running_val += loss.item() * y.size(0)\n",
    "        val_loss = running_val / len(ds_va)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        print(f\"Epoch {ep:03d} | train {train_loss:.4f} | val {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save({'model': model.state_dict(), 'spec_len': spec_len}, ARTIFACTS_DIR / 'best_simple.pt')\n",
    "\n",
    "    return {'model_state': model.state_dict(), 'history': history, 'spec_len': spec_len}\n",
    "\n",
    "# -----------------------------\n",
    "# Prediction using combined_real_gain0\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def predict_with_real(val_npz: str = VAL_NPZ, checkpoint: str = ARTIFACTS_DIR / 'best_simple.pt') -> Tuple[np.ndarray, np.ndarray]:\n",
    "    ckpt = np.load(checkpoint, allow_pickle=True) if str(checkpoint).endswith('.npz') else None\n",
    "    # load torch checkpoint\n",
    "    import torch\n",
    "    ck = torch.load(checkpoint, map_location=DEVICE)\n",
    "    spec_len = ck.get('spec_len') if isinstance(ck, dict) else None\n",
    "\n",
    "    # load freq grid and mask\n",
    "    freq_grid = load_freq_grid()\n",
    "    mask = build_hard_mask(freq_grid, LOW_HZ, HIGH_HZ)\n",
    "\n",
    "    # load real spec\n",
    "    real_spec = np.load(REAL_SPEC_NPY).astype(np.float32)\n",
    "    real_spec = normalize_spec_db_to_peak(real_spec)\n",
    "    real_spec = real_spec * mask\n",
    "\n",
    "    # load validation waveforms\n",
    "    data = np.load(val_npz)\n",
    "    waveforms = data['waveforms'].astype(np.float32)\n",
    "    counts_true = data['counts'].astype(np.int64)\n",
    "\n",
    "    # build model and load weights\n",
    "    spec_len_detected = real_spec.shape[0]\n",
    "    model = SimpleClassifier(spec_len=spec_len_detected).to(DEVICE)\n",
    "    model.load_state_dict(ck['model'])\n",
    "    model.eval()\n",
    "\n",
    "    # prepare batches\n",
    "    N = waveforms.shape[0]\n",
    "    batch_size = 128\n",
    "    preds = np.zeros((N,), dtype=np.int64)\n",
    "    probs = np.zeros((N, NUM_CLASSES), dtype=np.float32)\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        chunk = waveforms[i:i+batch_size]\n",
    "        wf_t = np.stack([standardize_waveform(w) for w in chunk], axis=0)\n",
    "        wf_t = torch.from_numpy(wf_t)[:, None, :].float().to(DEVICE)\n",
    "        spec_t = torch.from_numpy(np.tile(real_spec[None, :], (wf_t.size(0), 1))).float().to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            logits = model(wf_t, spec_t)\n",
    "            p = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "        probs[i:i+batch_size] = p\n",
    "        preds[i:i+batch_size] = p.argmax(axis=-1)\n",
    "\n",
    "    # save\n",
    "    np.save(ARTIFACTS_DIR / 'pred_class_probs_real_gain0.npy', probs)\n",
    "    np.save(ARTIFACTS_DIR / 'pred_counts_real_gain0.npy', preds)\n",
    "    return preds, probs\n",
    "\n",
    "# -----------------------------\n",
    "# Plot helpers for notebook\n",
    "# -----------------------------\n",
    "def plot_losses(history: Dict[str, list]):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(history['train_loss'], label='train')\n",
    "    plt.plot(history['val_loss'], label='val')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion(y_true: np.ndarray, y_pred: np.ndarray, normalize: bool = True, vmax: float = None):\n",
    "    K = NUM_CLASSES\n",
    "    cm = np.zeros((K,K), dtype=np.int64)\n",
    "    for t,p in zip(y_true, y_pred):\n",
    "        cm[int(t), int(p)] += 1\n",
    "    if normalize:\n",
    "        row_sums = cm.sum(axis=1, keepdims=True).astype(np.float32)\n",
    "        cm_norm = np.divide(cm, row_sums, out=np.zeros_like(cm, dtype=float), where=row_sums!=0)\n",
    "        m = cm_norm\n",
    "    else:\n",
    "        m = cm\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.imshow(m, origin='lower', aspect='auto', vmax=vmax)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('predicted')\n",
    "    plt.ylabel('true')\n",
    "    plt.title('Confusion matrix (rows=true)')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pred_vs_true_hist(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.hist(y_true, bins=np.arange(NUM_CLASSES+1)-0.5, alpha=0.5, label='true')\n",
    "    plt.hist(y_pred, bins=np.arange(NUM_CLASSES+1)-0.5, alpha=0.5, label='pred')\n",
    "    plt.xlabel('counts')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# End of script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c613e59-5ead-4a8c-8139-90117d71cf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple Jupyter-friendly Sim→Real counter predictor (minimal, opinionated)\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "Objetivo\n",
    "========\n",
    "Entrenar con (waveform_muon, gain0_sim) -> counts (clase 0..20).\n",
    "En test reemplazar gain0_sim por combined_real_gain0 y predecir los counts esperados.\n",
    "\n",
    "Principios de diseño (simplificados a pedido):\n",
    "- Espectros en dB, normalizados a pico = 0 dB (siempre).\n",
    "- Enmascaramiento DURO fuera de [1e3, 1e9] Hz (se multiplican por 0).\n",
    "- Sin augmentations ni randomizaciones.\n",
    "- No hay resampling en el código: **asumo que gain0_sim y combined_real_gain0 ya están en la MISMA grilla**\n",
    "  y que esa grilla está guardada en `data/freq_grid_hz.npy` (Hz). Si no la tenés, creala\n",
    "  con la misma longitud que tus espectros. Esto evita complejidad en el script.\n",
    "- Salida: clasificación con 21 clases (0..20).\n",
    "- Código pensado para ejecutarse en un notebook: funciones que devuelven historiales y permiten\n",
    "  graficar losses, comparar distribuciones y visualizar la matriz de confusión.\n",
    "\n",
    "Archivos esperados (poner en data/):\n",
    "- sim_train.npz  -> keys: 'waveforms' (N,T), 'gain0_sim' (N,F, dB), 'counts' (N) integers 0..20\n",
    "- sim_val.npz    -> same structure (validación)\n",
    "- combined_real_gain0.npy -> shape (F,) en dB (misma grilla que gain0_sim)\n",
    "- freq_grid_hz.npy -> shape (F,) con frecuencias en Hz (misma malla para los espectros)\n",
    "\n",
    "Salida producida:\n",
    "- artifacts/best_simple.pt                 (modelo guardado)\n",
    "- artifacts/pred_counts_real_gain0.npy     (predicciones sobre val usando combined_real_gain0)\n",
    "- functions devuelven arrays para plotear en notebook\n",
    "\n",
    "Instrucciones de uso (notebook):\n",
    "1) Cargá este script como módulo o pegalo en una celda.\n",
    "2) Llamá a `train_model(...)` para entrenar y recibir `history`.\n",
    "3) Usá `plot_losses(history)` y `plot_confusion(y_true, y_pred)` para visualizar.\n",
    "4) Ejecutá `predict_with_real(...)` para obtener predicciones usando `combined_real_gain0`.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "# -----------------------------\n",
    "# Minimal user-editable variables\n",
    "# -----------------------------\n",
    "TRAIN_NPZ = 'data/sim_train.npz'\n",
    "VAL_NPZ = 'data/sim_val.npz'\n",
    "REAL_SPEC_NPY = 'data/combined_real_gain0.npy'\n",
    "FREQ_GRID_NPY = 'data/freq_grid_hz.npy'   # required: same length as spec\n",
    "ARTIFACTS_DIR = Path('artifacts')\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "\n",
    "# Frequency bounds (hard mask)\n",
    "LOW_HZ = 1e3\n",
    "HIGH_HZ = 1e9\n",
    "\n",
    "# Fixed problem dimensions (will be inferred from data on load)\n",
    "NUM_CLASSES = 21  # classes 0..20\n",
    "\n",
    "# -----------------------------\n",
    "# Utility functions\n",
    "# -----------------------------\n",
    "\n",
    "def load_freq_grid():\n",
    "    p = Path(FREQ_GRID_NPY)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Freq grid file not found: {p}. Create a freq_grid_hz.npy matching your spectra\")\n",
    "    return np.load(p)\n",
    "\n",
    "\n",
    "def build_hard_mask(freq_grid_hz: np.ndarray, low_hz: float = LOW_HZ, high_hz: float = HIGH_HZ) -> np.ndarray:\n",
    "    mask = np.ones_like(freq_grid_hz, dtype=np.float32)\n",
    "    mask[(freq_grid_hz < low_hz) | (freq_grid_hz > high_hz)] = 0.0\n",
    "    return mask\n",
    "\n",
    "\n",
    "def normalize_spec_db_to_peak(spec_db: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Subtract max so peak is 0 dB. Works on 1D arrays or batch arrays (last axis = freq).\"\"\"\n",
    "    spec_db = np.array(spec_db, dtype=np.float32)\n",
    "    if spec_db.ndim == 1:\n",
    "        return spec_db - np.max(spec_db)\n",
    "    else:\n",
    "        return spec_db - np.max(spec_db, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "def standardize_waveform(wf: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    wf = np.array(wf, dtype=np.float32)\n",
    "    m = wf.mean()\n",
    "    s = wf.std()\n",
    "    return (wf - m) / (s + eps)\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset\n",
    "# -----------------------------\n",
    "class SimpleSimDataset(Dataset):\n",
    "    def __init__(self, npz_path: str, freq_mask: np.ndarray):\n",
    "        data = np.load(npz_path)\n",
    "        self.waveforms = data['waveforms'].astype(np.float32)  # [N,T]\n",
    "        self.spec = data['gain0_sim'].astype(np.float32)      # [N,F] in dB\n",
    "        self.counts = data['counts'].astype(np.int64)         # [N]\n",
    "        assert self.waveforms.ndim == 2\n",
    "        assert self.spec.ndim == 2\n",
    "        assert self.waveforms.shape[0] == self.spec.shape[0] == self.counts.shape[0]\n",
    "        self.freq_mask = freq_mask.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.waveforms.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wf = standardize_waveform(self.waveforms[idx])  # [T]\n",
    "        spec = normalize_spec_db_to_peak(self.spec[idx]) # [F]\n",
    "        spec = spec * self.freq_mask\n",
    "        y = int(self.counts[idx])\n",
    "        if y < 0:\n",
    "            y = 0\n",
    "        if y >= NUM_CLASSES:\n",
    "            y = NUM_CLASSES - 1\n",
    "        wf_t = torch.from_numpy(wf)[None, :].float()   # [1,T]\n",
    "        spec_t = torch.from_numpy(spec).float()        # [F]\n",
    "        return wf_t, spec_t, torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# -----------------------------\n",
    "# Model (small, simple)\n",
    "# -----------------------------\n",
    "class WaveformEncoder(nn.Module):\n",
    "    def __init__(self, in_ch: int = 1, out_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 32, kernel_size=9, stride=2, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, out_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SpecEncoder(nn.Module):\n",
    "    def __init__(self, f_len: int, out_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(f_len, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, out_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, spec_len: int, wf_out: int = 64, spec_out: int = 64, n_classes: int = NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        self.wf_enc = WaveformEncoder(in_ch=1, out_dim=wf_out)\n",
    "        self.spec_enc = SpecEncoder(f_len=spec_len, out_dim=spec_out)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(wf_out + spec_out, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, wf, spec):\n",
    "        # wf: [B,1,T], spec: [B,F]\n",
    "        wf_feat = self.wf_enc(wf)\n",
    "        spec_feat = self.spec_enc(spec)\n",
    "        x = torch.cat([wf_feat, spec_feat], dim=-1)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "# -----------------------------\n",
    "# Training / utilities for notebook\n",
    "# -----------------------------\n",
    "\n",
    "def train_model(train_npz: str = TRAIN_NPZ, val_npz: str = VAL_NPZ, epochs: int = EPOCHS, batch_size: int = BATCH_SIZE, lr: float = LR) -> Dict[str, Any]:\n",
    "    # load freq grid and build mask\n",
    "    freq_grid = load_freq_grid()\n",
    "    mask = build_hard_mask(freq_grid, LOW_HZ, HIGH_HZ)\n",
    "\n",
    "    # datasets and loaders\n",
    "    ds_tr = SimpleSimDataset(train_npz, mask)\n",
    "    ds_va = SimpleSimDataset(val_npz, mask)\n",
    "    spec_len = ds_tr.spec.shape[1]\n",
    "\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = SimpleClassifier(spec_len=spec_len).to(DEVICE)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    best_val = float('inf')\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for wf, spec, y in dl_tr:\n",
    "            wf = wf.to(DEVICE)\n",
    "            spec = spec.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            logits = model(wf, spec)\n",
    "            loss = loss_fn(logits, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            running_loss += loss.item() * y.size(0)\n",
    "        train_loss = running_loss / len(ds_tr)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        running_val = 0.0\n",
    "        with torch.no_grad():\n",
    "            for wf, spec, y in dl_va:\n",
    "                wf = wf.to(DEVICE)\n",
    "                spec = spec.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "                logits = model(wf, spec)\n",
    "                loss = loss_fn(logits, y)\n",
    "                running_val += loss.item() * y.size(0)\n",
    "        val_loss = running_val / len(ds_va)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        print(f\"Epoch {ep:03d} | train {train_loss:.4f} | val {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save({'model': model.state_dict(), 'spec_len': spec_len}, ARTIFACTS_DIR / 'best_simple.pt')\n",
    "\n",
    "    return {'model_state': model.state_dict(), 'history': history, 'spec_len': spec_len}\n",
    "\n",
    "# -----------------------------\n",
    "# Prediction using combined_real_gain0\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def predict_with_real(val_npz: str = VAL_NPZ, checkpoint: str = ARTIFACTS_DIR / 'best_simple.pt') -> Tuple[np.ndarray, np.ndarray]:\n",
    "    ckpt = np.load(checkpoint, allow_pickle=True) if str(checkpoint).endswith('.npz') else None\n",
    "    # load torch checkpoint\n",
    "    import torch\n",
    "    ck = torch.load(checkpoint, map_location=DEVICE)\n",
    "    spec_len = ck.get('spec_len') if isinstance(ck, dict) else None\n",
    "\n",
    "    # load freq grid and mask\n",
    "    freq_grid = load_freq_grid()\n",
    "    mask = build_hard_mask(freq_grid, LOW_HZ, HIGH_HZ)\n",
    "\n",
    "    # load real spec\n",
    "    real_spec = np.load(REAL_SPEC_NPY).astype(np.float32)\n",
    "    real_spec = normalize_spec_db_to_peak(real_spec)\n",
    "    real_spec = real_spec * mask\n",
    "\n",
    "    # load validation waveforms\n",
    "    data = np.load(val_npz)\n",
    "    waveforms = data['waveforms'].astype(np.float32)\n",
    "    counts_true = data['counts'].astype(np.int64)\n",
    "\n",
    "    # build model and load weights\n",
    "    spec_len_detected = real_spec.shape[0]\n",
    "    model = SimpleClassifier(spec_len=spec_len_detected).to(DEVICE)\n",
    "    model.load_state_dict(ck['model'])\n",
    "    model.eval()\n",
    "\n",
    "    # prepare batches\n",
    "    N = waveforms.shape[0]\n",
    "    batch_size = 128\n",
    "    preds = np.zeros((N,), dtype=np.int64)\n",
    "    probs = np.zeros((N, NUM_CLASSES), dtype=np.float32)\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        chunk = waveforms[i:i+batch_size]\n",
    "        wf_t = np.stack([standardize_waveform(w) for w in chunk], axis=0)\n",
    "        wf_t = torch.from_numpy(wf_t)[:, None, :].float().to(DEVICE)\n",
    "        spec_t = torch.from_numpy(np.tile(real_spec[None, :], (wf_t.size(0), 1))).float().to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            logits = model(wf_t, spec_t)\n",
    "            p = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "        probs[i:i+batch_size] = p\n",
    "        preds[i:i+batch_size] = p.argmax(axis=-1)\n",
    "\n",
    "    # save\n",
    "    np.save(ARTIFACTS_DIR / 'pred_class_probs_real_gain0.npy', probs)\n",
    "    np.save(ARTIFACTS_DIR / 'pred_counts_real_gain0.npy', preds)\n",
    "    return preds, probs\n",
    "\n",
    "# -----------------------------\n",
    "# Plot helpers for notebook\n",
    "# -----------------------------\n",
    "def plot_losses(history: Dict[str, list]):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(history['train_loss'], label='train')\n",
    "    plt.plot(history['val_loss'], label='val')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion(y_true: np.ndarray, y_pred: np.ndarray, normalize: bool = True, vmax: float = None):\n",
    "    K = NUM_CLASSES\n",
    "    cm = np.zeros((K,K), dtype=np.int64)\n",
    "    for t,p in zip(y_true, y_pred):\n",
    "        cm[int(t), int(p)] += 1\n",
    "    if normalize:\n",
    "        row_sums = cm.sum(axis=1, keepdims=True).astype(np.float32)\n",
    "        cm_norm = np.divide(cm, row_sums, out=np.zeros_like(cm, dtype=float), where=row_sums!=0)\n",
    "        m = cm_norm\n",
    "    else:\n",
    "        m = cm\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.imshow(m, origin='lower', aspect='auto', vmax=vmax)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('predicted')\n",
    "    plt.ylabel('true')\n",
    "    plt.title('Confusion matrix (rows=true)')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pred_vs_true_hist(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.hist(y_true, bins=np.arange(NUM_CLASSES+1)-0.5, alpha=0.5, label='true')\n",
    "    plt.hist(y_pred, bins=np.arange(NUM_CLASSES+1)-0.5, alpha=0.5, label='pred')\n",
    "    plt.xlabel('counts')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# End of script\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
